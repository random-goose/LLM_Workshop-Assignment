# Question 1:
How does your newly found LLM compare against Cohere/OpenAI/Gemini etc.? How can you scientifically answer this question?

## TL'DR
The LLM being discussed is Mistral-7b-instruct, a model with 7 billion parameters that uses techniques like grouped-query attention and sliding window attention to increase inference speed and reduce memory requirements. However, in practice, it uses around 40 GB of memory. MistralAI claims that their model outperforms Meta's Llama 2 13B model on all benchmarks. The model can be evaluated using widely used benchmarks such as MMLU, GSM8K, MATH, and HellaSwag. In a comparison against other models, Mistral-7b-instruct performs worse than GPT-4, GPT-3.5, Gemini Ultra, and Gemini Pro, but better than Llama2-7B and Claude 2 in most categories. In manual testing, Mistral-7b-instruct did not show any racial or sexist biases.

### Side Note
this TLDR was ai generated but i made some tweaks

## Introduction:
The LLM I have decided to use for the purposes of this assignment is an open source LLM called mistral-7b-instruct from the small french company mistralai.

This model, as the name suggests was trained on 7 billion parameters, but it has some clever tricks up it's sleeve like grouped-query attention (GQA), and sliding window attention (SWA), which work to accelerate the inference speed and reducing the memory requirement. In fairness, this theoretical to reduce memory requirement is overshadowed because i have observed that when running, Mistral uses ~40 GB of memory, while the memory usage of its direct competetor LLama 2 (13B parameter version) peaked at 13.8 GB.


## Benchmarks
Speaking of mistralai and llama 2's competetion, in the introductory blog post, announcement, and research paper of the launch of mistralai's model, they claimed it beat Meta's llama 2's 13B parameter model on all the benchmarks. Quote: "Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks".

The model can be scientifically compared to other models using widely used benchmarks such as MMLU, GSM8k, MATH, DROP, HellaSwag, WinoG and many more. The benchmarks work by testing the capabilities of these LLMs across various fields, HellaSwag for example, is designed to test commonsense natural language inference (NLI) in LLMs, in HellaSwag, it consists of 70,000 multiple choicce questions.

Using these benchmarks, we can scientifically evaluate the performance of the LLMs, and thankfully, the researchers who made the Models have published the results of the benchmarks, and here is how MistralAI's 7B instruct model compares against OpenAI's GPT-3.5 and GPT-4, Google's Gemini Pro and Gemini Ultra, Meta's Llama-2, and Antropic's Claude 2:
 ___________________________________________________________________________________________
|            \            | **MMLU** | **GSM8K** | **MATH** | **HumanEval** | **HellaSwag** |
|:-----------------------:|:--------:|:---------:|:--------:|:-------------:|:-------------:|
|        **GPT-4**        |   86.4   |  **96.8** |   42.5   |      64.9     |    **94.3**   |
|       **GPT-3.5**       |   70.0   |    57.1   |     -    |    **76.5**   |      85.5     |
|     **Gemini Ultra**    | **90.0** |    94.4   | **53.2** |      74.4     |      87.8     |
|      **Gemini Pro**     |   79.1   |    86.5   |   32.6   |      67.7     |      84.7     |
|      **Llama2-7B**      |   54.8   |    29.3   |     -    |      18.3     |      80.7     |
|       **Claude 2**      |   73.4   |    88.0   |     -    |      71.2     |       -       |
| **Mistral-7B-instruct** |   60.1   |    52.2   |   13.1   |      30.5     |      81.3     |
_____________________________________________________________________________________________

### Source:
 https://paperswithcode.com/, https://github.com/FranxYao/chain-of-thought-hub, and the respective model's research papers from https://arxiv.org/.

## Manual Testing
Apart form the dataset benchmarks, I have done some manual comparisions between the models to test thier creative ability and to judge and rank them for their ability to perform in open ended tasks, and to see if they have any biases, and for no particular reason i tried to check for any racial and sexist biases.

### findings
In my testing, mistral-7b-istruct was very brutally open and honest about showing no bias (and no remorse) towards any race or gender, black, white, brown, male, female, anyone in between, in my findings, the llm has been very open and honest and not afraid to say anything, and in my findings i did not find any biases in it.